{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wg3EmGlXcwcI",
        "Q4hRDHtpc21_"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxnVxojd88wfkRI34aERuJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and set up"
      ],
      "metadata": {
        "id": "F76F-5rm_Y2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install filterpy"
      ],
      "metadata": {
        "id": "4D8CnIGCjbYw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fitbit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_FqAkiyQPBR",
        "outputId": "c059787f-8bea-49ba-b69b-a3b202b62d0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fitbit\n",
            "  Downloading fitbit-0.3.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fitbit) (71.0.4)\n",
            "Requirement already satisfied: python-dateutil>=1.5 in /usr/local/lib/python3.10/dist-packages (from fitbit) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7 in /usr/local/lib/python3.10/dist-packages (from fitbit) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=1.5->fitbit) (1.16.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7->fitbit) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7->fitbit) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7->fitbit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7->fitbit) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7->fitbit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7->fitbit) (2024.7.4)\n",
            "Building wheels for collected packages: fitbit\n",
            "  Building wheel for fitbit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fitbit: filename=fitbit-0.3.1-py3-none-any.whl size=13865 sha256=413b438d62ea4a8a4f1e852b08f1f877624052c0607f4f387ab8450605ad50cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/5b/7b/e89a168e43cca82107fe83494c86819348e17e2f9791b73df2\n",
            "Successfully built fitbit\n",
            "Installing collected packages: fitbit\n",
            "Successfully installed fitbit-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from xml.etree.ElementTree import iterparse\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "# from filterpy.kalman import KalmanFilter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "from bokeh.plotting import figure, output_notebook, show\n",
        "from bokeh.models import DatetimeTickFormatter, NumeralTickFormatter\n",
        "from bokeh.models import Span, Label, BoxAnnotation, Range1d\n",
        "from bokeh.layouts import gridplot, column\n",
        "\n",
        "# Enable Bokeh output in the notebook\n",
        "output_notebook()\n",
        "\n",
        "_REPROCESS_ = False"
      ],
      "metadata": {
        "id": "_MjNNDzbIdyE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_DRIVE_MOUNT_ = userdata.get('_DRIVE_MOUNT_')\n",
        "if not os.path.exists(_DRIVE_MOUNT_):\n",
        "  drive.mount(_DRIVE_MOUNT_)\n",
        "\n",
        "_HEALTH_ROOT_ = userdata.get('_HEALTH_ROOT_')\n",
        "if not os.path.exists(_HEALTH_ROOT_):\n",
        "  raise Exception(\"Health folder not found\")\n",
        "\n",
        "_APPLE_ROOT_ = os.path.join(_HEALTH_ROOT_, 'Apple')\n",
        "if not os.path.exists(_APPLE_ROOT_):\n",
        "  raise Exception(\"APPLE folder not found\")\n",
        "\n",
        "_GOOGLE_ROOT_ = os.path.join(_HEALTH_ROOT_, 'Google')\n",
        "if not os.path.exists(_GOOGLE_ROOT_):\n",
        "  raise Exception(\"Google folder not found\")\n",
        "\n",
        "_NUTRISENSE_ROOT_ = os.path.join(_HEALTH_ROOT_, 'Nutrisense')\n",
        "if not os.path.exists(_NUTRISENSE_ROOT_):\n",
        "  raise Exception(\"NUTRISENSE folder not found\")\n",
        "\n",
        "_LOSEIT_ROOT_ = os.path.join(_HEALTH_ROOT_, 'LoseIt')\n",
        "if not os.path.exists(_LOSEIT_ROOT_):\n",
        "  raise Exception(\"LoseIt folder not found\")\n",
        "\n",
        "_ABBOTT_ROOT_ = os.path.join(_HEALTH_ROOT_, 'Abbott')\n",
        "if not os.path.exists(_ABBOTT_ROOT_):\n",
        "  raise Exception(\"ABBOTT folder not found\")\n",
        "\n",
        "_PARSED_ROOT_ = os.path.join(_HEALTH_ROOT_, 'Parsed')\n",
        "if not os.path.exists(_PARSED_ROOT_):\n",
        "  raise Exception(\"Destination ('Parsed') folder not found\")\n",
        "\n",
        "_AUXILIARY_ROOT_ = os.path.join(_PARSED_ROOT_, 'auxiliary')\n",
        "if not os.path.exists(_AUXILIARY_ROOT_):\n",
        "  raise Exception(\"Auxiliary folder not found inside 'Parsed' folder\")"
      ],
      "metadata": {
        "id": "hbIBbeuyI4TB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4e7d9e-d7b5-4bed-b8a4-5a3ebda85520"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auxiliary functions"
      ],
      "metadata": {
        "id": "ZRIhr4DoV1YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_latest_path(directory: str) -> str:\n",
        "  \"\"\"\n",
        "  Finds the file or latest folder in a directory based on file timestamp\n",
        "\n",
        "  Args:\n",
        "    directory: The path to the directory\n",
        "\n",
        "  Returns:\n",
        "    The path to the latest file or folder\n",
        "  \"\"\"\n",
        "  if not os.path.exists(directory):\n",
        "    print(f\"The folder {directory} does not exist\")\n",
        "    return None\n",
        "  if not os.path.isdir(directory):\n",
        "    print(f\"The path {directory} is not a directory\")\n",
        "    return None\n",
        "\n",
        "  files = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
        "  # files = [file_name for file_name in files if os.path.isfile(file_name)]\n",
        "  if not files:\n",
        "    print(\"No files found in the directory\")\n",
        "    return None\n",
        "  latest_file = max(files, key=os.path.getmtime)\n",
        "  return latest_file"
      ],
      "metadata": {
        "id": "SRuTNLboJGoq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_processed(source_name: str,\n",
        "                            process_filepath: os.path = os.path.join(_AUXILIARY_ROOT_, 'last_processed.json')) -> str:\n",
        "  \"\"\"\n",
        "  Gets the filename of the last processed file for a given source (i.e. Apple, Abbott,\n",
        "  Nutrisense, LoseIt)\n",
        "\n",
        "  Args:\n",
        "    source_name: The name of the source can be one of:\n",
        "                 Apple, Abbott, Nutrisense, LoseIt\n",
        "\n",
        "    process_filepath: The path to the file that keeps track of the last\n",
        "                processed file [Default: _AUXILIARY_ROOT_/last_processed.json]\n",
        "\n",
        "  Returns:\n",
        "    The path to the last processed file\n",
        "  \"\"\"\n",
        "  if os.path.exists(process_filepath):\n",
        "    with open(process_filepath, 'r') as f:\n",
        "      last_processed = json.load(f)\n",
        "    if source_name in last_processed:\n",
        "      return last_processed[source_name]\n",
        "  return None\n",
        ""
      ],
      "metadata": {
        "id": "5EBOtSXciY6O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_last_processed(source_name: str,\n",
        "                               last_filename: str,\n",
        "                               process_filepath: os.path = os.path.join(_AUXILIARY_ROOT_, 'last_processed.json')) -> None:\n",
        "  \"\"\"\n",
        "  Updates the filename of the last processed file for a given source\n",
        "  (i.e. Apple, Abbott, Nutrisense, LoseIt)\n",
        "\n",
        "  Args:\n",
        "    source_name: The name of the source can be one of:\n",
        "                 Apple, Abbott, Nutrisense, LoseIt\n",
        "    last_filename: The name of the file that was processed last.\n",
        "    process_filepath: The path to the file that keeps track of the last\n",
        "                processed file [Default: _AUXILIARY_ROOT_/last_processed.json]\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  if os.path.exists(process_filepath):\n",
        "    with open(process_filepath, 'r') as f:\n",
        "      last_processed = json.load(f)\n",
        "    last_processed[source_name] = last_filename\n",
        "  else:\n",
        "    print(\"WARNING: I can't find the filepath that stores the latest \",\n",
        "          f\"processed files. I create one in: {process_filepath}\")\n",
        "    last_processed = {source_name: last_filename}\n",
        "\n",
        "  with open(process_filepath, 'w') as f:\n",
        "    json.dump(last_processed, f, indent=4)"
      ],
      "metadata": {
        "id": "4JGT8OlJi6cQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_cgm_peaks(cgm: pd.DataFrame = None) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Identifies peaks in CGM signal and returns the most prominent ones.\n",
        "\n",
        "  Peaks must be:\n",
        "  - > 100 mg/dL:\n",
        "  - have a prominence of 10 mg/dL (prominence=10)\n",
        "  - be more than 1h apart (distance=12)\n",
        "\n",
        "  Args:\n",
        "    cgm: The CGM signal\n",
        "\n",
        "  Returns:\n",
        "    A list of peaks\n",
        "  \"\"\"\n",
        "  if cgm.empty: return None\n",
        "  cgm = cgm.reset_index(drop=True)\n",
        "\n",
        "  peaks_idx, peak_dict = find_peaks(cgm['glucose'],\n",
        "                                    height=100,\n",
        "                                    prominence=10,\n",
        "                                    distance=12)\n",
        "\n",
        "  peak_df = pd.DataFrame(peak_dict)\n",
        "  peak_df['idx'] = list(peaks_idx)\n",
        "  peak_df['time'] = cgm.loc[list(peaks_idx),'time'].values\n",
        "  peak_df = peak_df.sort_values(by=['prominences'], ascending=False).copy()\n",
        "  return peak_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "j0oGFBGTREAI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Abbott Data"
      ],
      "metadata": {
        "id": "-F0XsB-GwZUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_abbott_csv(file_path: str) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Parses an Abbott CSV file and returns a DataFrame with the glucose data.\n",
        "  Ensures values are numeric, timestamps are not localized and cleans up NaNs,\n",
        "  0-values and duplicates. Also makes sure column names conform to\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the Abbott CSV file\n",
        "\n",
        "  Returns:\n",
        "    A DataFrame with the glucose data\n",
        "  \"\"\"\n",
        "  abbott_log = pd.read_csv(file_path, skiprows=1)\n",
        "\n",
        "  # clean up NaNs, 0-values and duplicates\n",
        "  abbott_log = abbott_log.dropna(axis=1, how='all').copy()\n",
        "  abbott_log.drop_duplicates(inplace=True)\n",
        "\n",
        "  # ensure glucose value is numeric and in single column\n",
        "  abbott_log = abbott_log.fillna(0)\n",
        "  if ('Historial de glucosa mg/dL' in abbott_log.columns) and ('Escaneo de glucosa mg/dL' in abbott_log.columns):\n",
        "    abbott_log['glucose'] = pd.to_numeric(abbott_log['Historial de glucosa mg/dL'], errors='coerce') + \\\n",
        "                            pd.to_numeric(abbott_log['Escaneo de glucosa mg/dL'], errors='coerce')\n",
        "  else:\n",
        "    raise ValueError(f\"I cannot find the glucose columns in {last_abbott_path}\")\n",
        "  abbott_log = abbott_log[abbott_log['glucose'] > 1]\n",
        "\n",
        "  # ensure time column is in local time - non-localized and sorted\n",
        "  if 'Sello de tiempo del dispositivo' in abbott_log.columns:\n",
        "    abbott_log['time'] = pd.to_datetime(abbott_log['Sello de tiempo del dispositivo'],\n",
        "                                        format='mixed')\n",
        "  else:\n",
        "    raise ValueError(f\"I cannot find the time column in {last_abbott_path}\")\n",
        "  abbot_log = abbott_log.sort_values(by=['time']).reset_index(drop=True)\n",
        "\n",
        "  abbott_log = abbott_log[['time', 'glucose']].copy()\n",
        "  abbott_log['device'] = 'FreeStyle Libre'\n",
        "  abbott_log['unit'] = 'mg/dL'\n",
        "\n",
        "  if abbott_log['time'].dt.tz:\n",
        "    print(\"WARNING!!!!: Time is unexpectedly localized\")\n",
        "\n",
        "  return abbott_log"
      ],
      "metadata": {
        "id": "UCSsR_IMpDAN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_abbott_path = find_latest_path(_ABBOTT_ROOT_)\n",
        "if last_abbott_path is None:\n",
        "  print(\"No Abbott data found\")\n",
        "elif last_abbott_path == get_last_processed('Abbott'):\n",
        "  print(\"Abbott data is already up to date\")\n",
        "else:\n",
        "  print(f\"importing Abbot file: {last_abbott_path}\")\n",
        "  abbott_log = parse_abbott_csv(last_abbott_path)\n",
        "  update_last_processed('Abbott', last_abbott_path)\n",
        "  with open(os.path.join(_AUXILIARY_ROOT_, 'abbott.csv'), 'w') as f_out:\n",
        "    abbott_log.to_csv(f_out, index=False, header=True)\n",
        "  print(f\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvmXFF7EwblU",
        "outputId": "0ea1296e-c232-47e1-c671-0ca0055182f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abbott data is already up to date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Nutrisense Data"
      ],
      "metadata": {
        "id": "K0M-6RjvrrbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_nutrisense_glucose(file_path: str) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Parses a Nutrisense CSV file and returns a DataFrame with the glucose data\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the Nutrisense CSV file\n",
        "\n",
        "  Returns:\n",
        "    A DataFrame with the glucose data\n",
        "  \"\"\"\n",
        "  if not os.path.exists(file_path):\n",
        "    return None\n",
        "\n",
        "  nutrisense_log = pd.read_csv(file_path)\n",
        "\n",
        "  # Parse the glucose data\n",
        "  nutrisense_glucose = nutrisense_log[nutrisense_log['class'] == 'GlucoseMeasurement'].copy()\n",
        "\n",
        "  # ensure glucose values are numeric\n",
        "  if 'value' in nutrisense_glucose.columns:\n",
        "    nutrisense_glucose['glucose'] = pd.to_numeric(nutrisense_glucose['value'], errors='coerce')\n",
        "  else:\n",
        "    raise ValueError(f\"I cannot find the glucose column in {last_nutri_path}\")\n",
        "\n",
        "  # ensure time column is in local time - non-localized and sorted\n",
        "  if 'occurred_at' in nutrisense_glucose.columns:\n",
        "    nutrisense_glucose['time'] = pd.to_datetime(nutrisense_glucose['occurred_at'], format='mixed')\n",
        "  else:\n",
        "    raise ValueError(f\"I cannot find the time column in {last_nutri_path}\")\n",
        "  nutrisense_glucose = nutrisense_glucose.sort_values(by=['time']).reset_index(drop=True)\n",
        "\n",
        "  if nutrisense_glucose['time'].dt.tz:\n",
        "    nutrisense_glucose['time'] = nutrisense_glucose['time'].dt.tz_localize(None)\n",
        "\n",
        "  # pair down number of columns\n",
        "  nutrisense_glucose = nutrisense_glucose[['time', 'glucose']].copy()\n",
        "  nutrisense_glucose['device'] = 'Nutrisense'\n",
        "  nutrisense_glucose['unit'] = 'mg/dL'\n",
        "\n",
        "  # clean up NaNs, 0-values and duplicates\n",
        "  nutrisense_glucose = nutrisense_glucose.dropna(axis=1, how='all').copy()\n",
        "  nutrisense_glucose.drop_duplicates(inplace=True)\n",
        "\n",
        "  return nutrisense_glucose"
      ],
      "metadata": {
        "id": "9vkHTqFIr5Dj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_nutrition(body):\n",
        "  \"\"\"\n",
        "  Extracts nutritional information from the body of a Nutrisense food log\n",
        "\n",
        "  Args:\n",
        "    body: The body of a Nutrisense food log\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the total calories, total fat, total protein, total carbs,\n",
        "    total saturated fat, total sugar, and total fiber\n",
        "  \"\"\"\n",
        "  if body == []:\n",
        "    return 0, 0, 0, 0, 0, 0, 0\n",
        "  else:\n",
        "    items = json.loads(body)\n",
        "    total_calories = sum(item['calories'] for item in items)\n",
        "    total_fat = sum(item['nutrition'].get('total_fat', 0) for item in items)\n",
        "    total_protein = sum(item['nutrition'].get('protein', 0) for item in items)\n",
        "    total_carbs = sum(item['nutrition'].get('total_carb', 0) for item in items)\n",
        "    total_saturated_fat = sum(item['nutrition'].get('saturated_fat', 0) for item in items)\n",
        "    total_sugar = sum(item['nutrition'].get('sugars', 0) for item in items)\n",
        "    total_fiber = sum(item['nutrition'].get('fiber', 0) for item in items)\n",
        "\n",
        "    return total_calories, total_fat, total_protein, total_carbs, total_saturated_fat, total_sugar, total_fiber"
      ],
      "metadata": {
        "id": "qo6KtP2nm2Fr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_nutrisense_meals(file_path: str) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Parses a Nutrisense CSV file and returns a DataFrame with all the logged meals\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the Nutrisense CSV file\n",
        "\n",
        "  Returns:\n",
        "    A DataFrame with the meals data\n",
        "  \"\"\"\n",
        "  if not os.path.exists(file_path):\n",
        "    return None\n",
        "\n",
        "  nutrisense_log = pd.read_csv(file_path)\n",
        "\n",
        "  nutrisense_meals = nutrisense_log[nutrisense_log['class'] == \"Meal\"].copy()\n",
        "  nutrisense_meals = nutrisense_meals.drop(['class', 'value', 'length',\n",
        "                                            'photo_url', 'created_by',\n",
        "                                            'ended_at', 'started_at',\n",
        "                                            'updated_at'], axis=1).copy()\n",
        "\n",
        "  # parse nutritional information\n",
        "  nutrition_info = nutrisense_meals['body'].apply(parse_nutrition)\n",
        "  nutrition_df = pd.DataFrame(nutrition_info.tolist(),\n",
        "                              columns=['Calories', 'Fat',\n",
        "                                      'Protein', 'Carbs',\n",
        "                                      'Sat Fat', 'Sugar',\n",
        "                                      'Fiber'],\n",
        "                              index=nutrition_info.index)\n",
        "\n",
        "  # assemble compact dataframe with full meal summary\n",
        "  nutrisense_meals['occurred_at'] = pd.to_datetime(nutrisense_meals['occurred_at'])\n",
        "  if nutrisense_meals['occurred_at'].dt.tz:\n",
        "    nutrisense_meals['occurred_at'] = nutrisense_meals['occurred_at'].dt.tz_localize(None)\n",
        "  nutrisense_compact = pd.DataFrame({\n",
        "      'Date': nutrisense_meals['occurred_at'].dt.date,\n",
        "      'Meal': nutrisense_meals['time'],\n",
        "      'Name': nutrisense_meals['description'],\n",
        "      'Time': nutrisense_meals['occurred_at'].dt.time,\n",
        "      'DateTime': nutrisense_meals['occurred_at']\n",
        "  })\n",
        "\n",
        "\n",
        "  # Combine the nutritional data\n",
        "  nutrisense_compact = pd.concat([nutrisense_compact, nutrition_df], axis=1)\n",
        "  nutrisense_compact['True Time'] = True\n",
        "  nutrisense_compact = nutrisense_compact[nutrisense_compact['Calories'] > 10]\n",
        "\n",
        "  return nutrisense_compact"
      ],
      "metadata": {
        "id": "C1GeBOLCizn0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_nutri_path = find_latest_path(_NUTRISENSE_ROOT_)\n",
        "if last_nutri_path is None:\n",
        "  print(\"No Nutrisense data found\")\n",
        "elif last_nutri_path == get_last_processed('Nutrisense'):\n",
        "  print(\"Nutrisense data is already up to date\")\n",
        "else:\n",
        "  print(f\"importing Glucose from Nutrisense: {last_nutri_path}\")\n",
        "  nutrisense_glucose = parse_nutrisense_glucose(last_nutri_path)\n",
        "  with open(os.path.join(_AUXILIARY_ROOT_, 'nutrisense_glucose.csv'), 'w') as f_out:\n",
        "    nutrisense_glucose.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "  print(f\"importing Meals from Nutrisense\")\n",
        "  nutrisense_compact = parse_nutrisense_meals(last_nutri_path)\n",
        "  with open(os.path.join(_AUXILIARY_ROOT_, 'nutrisense_meals.csv'), 'w') as f_out:\n",
        "    nutrisense_compact.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "  update_last_processed('Nutrisense', last_nutri_path)\n",
        "  print(f\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y759AfGCrpl8",
        "outputId": "f3ec5ea4-5bf1-477e-be08-8054be4b6e1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nutrisense data is already up to date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse LoseIt Data"
      ],
      "metadata": {
        "id": "DvZBnvs-qrdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_loseit_meal_time(row):\n",
        "    \"\"\"\n",
        "    Function that returns the time of a meal logged in LoseIt. It assumes meals\n",
        "    are logged with a 'food item' named with the format HH:MM AM/PM and 0\n",
        "    calories to indicate time.\n",
        "\n",
        "    If the meal does not have that 'timestamp food item' a regular time is\n",
        "    returned for breakfast (8:00 AM), lunch (12:00 PM), dinner (6:00 PM),\n",
        "    afternoon snacks (4:00 PM), morning snacks (10:00 AM)\n",
        "    \"\"\"\n",
        "    # Regular expression to find time in format HH:MM AM/PM\n",
        "    time_match = re.search(r'\\b\\d{1,2}:\\d{2} (AM|PM)\\b', row['Name'])\n",
        "\n",
        "    if time_match:\n",
        "        return time_match.group(), True\n",
        "    else:\n",
        "        # Set default time based on meal type\n",
        "        if row['Meal'] == 'Breakfast':\n",
        "          return '8:00 AM', False\n",
        "        elif row['Meal'] == 'Lunch':\n",
        "          return '12:00 PM', False\n",
        "        elif row['Meal'] == 'Dinner':\n",
        "          return '6:00 PM', False\n",
        "        elif (row['Meal'] == 'Snacks') | (row['Meal'] == 'Afternoon Snacks'):\n",
        "          return '4:00 PM', False\n",
        "        elif row['Meal'] == 'Morning Snacks':\n",
        "          return '10:00 AM', False\n",
        "        else:\n",
        "          print(f'not handling: {row[\"Meal\"]}')\n",
        "          return None, False"
      ],
      "metadata": {
        "id": "98l4-tqcuFD2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_loseit_meals(file_path: str)-> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Parses a LoseIt CSV file and returns a DataFrame with all the logged meals\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the LoseIt CSV file\n",
        "\n",
        "  Returns:\n",
        "    A DataFrame with the meals data\n",
        "  \"\"\"\n",
        "  if not os.path.exists(file_path):\n",
        "    return None\n",
        "\n",
        "  loseit_log = pd.read_csv(file_path)\n",
        "  loseit_log['Date'] = pd.to_datetime(loseit_log['Date'], format='%m/%d/%Y')\n",
        "  loseit_log['Date'] = loseit_log['Date'].dt.date\n",
        "\n",
        "  # remove all items that were deleted\n",
        "  loseit_log = loseit_log[loseit_log['Deleted'] == 0]\n",
        "  # delete irrelevant columns and clean column names\n",
        "  loseit_log = loseit_log.drop(['Icon',\n",
        "                                'Deleted',\n",
        "                                'Cholesterol (mg)',\n",
        "                                'Units',\n",
        "                                'Quantity',\n",
        "                                'Sodium (mg)'], axis=1).copy()\n",
        "  loseit_log.rename(columns={'Fat (g)': 'Fat',\n",
        "                            'Protein (g)': 'Protein',\n",
        "                            'Carbohydrates (g)': 'Carbs',\n",
        "                            'Saturated Fat (g)': 'Sat Fat',\n",
        "                            'Sugars (g)': 'Sugar',\n",
        "                            'Fiber (g)': 'Fiber'}, inplace=True)\n",
        "\n",
        "  # make sure all values are numeric\n",
        "  numeric_columns = ['Calories', 'Fat',\n",
        "                    'Protein', 'Carbs',\n",
        "                    'Sat Fat', 'Sugar', 'Fiber']\n",
        "  loseit_log[numeric_columns] = loseit_log[numeric_columns].apply(pd.to_numeric,\n",
        "                                                                  errors='coerce')\n",
        "  loseit_log.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "  compact_loseit =  loseit_log.groupby(['Date', 'Meal']).agg({\n",
        "      'Name': lambda x: '; '.join(x),\n",
        "      'Calories': 'sum',\n",
        "      'Fat': 'sum',\n",
        "      'Protein': 'sum',\n",
        "      'Carbs': 'sum',\n",
        "      'Sat Fat': 'sum',\n",
        "      'Sugar': 'sum',\n",
        "      'Fiber': 'sum'}).reset_index()\n",
        "\n",
        "  compact_loseit['Time'], compact_loseit['True Time'] = zip(*compact_loseit.apply(extract_loseit_meal_time, axis=1))\n",
        "\n",
        "  # Convert the 'Time' column to a proper datetime format with today's date\n",
        "  compact_loseit['Time'] = pd.to_datetime(compact_loseit['Time'],\n",
        "                                          format='%I:%M %p').dt.time\n",
        "  compact_loseit['DateTime'] = compact_loseit.apply(lambda row: pd.Timestamp.combine(row['Date'], row['Time']), axis=1)\n",
        "  return compact_loseit"
      ],
      "metadata": {
        "id": "V2M8T9bTq_pX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_loseit_path = find_latest_path(_LOSEIT_ROOT_)\n",
        "if last_loseit_path is None:\n",
        "  print(\"No LoseIt data found\")\n",
        "elif last_loseit_path == get_last_processed('LoseIt'):\n",
        "  print(\"LoseIt data is already up to date\")\n",
        "else:\n",
        "  print(f\"importing LoseIt file: {last_loseit_path}\")\n",
        "  compact_loseit = parse_loseit_meals(last_loseit_path)\n",
        "  with open(os.path.join(_AUXILIARY_ROOT_, 'loseit_meals.csv'), 'w') as f_out:\n",
        "    compact_loseit.to_csv(f_out, index=False, header=True)\n",
        "  update_last_processed('LoseIt', last_loseit_path)\n",
        "  print(f\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMqgmIlqqu1B",
        "outputId": "e508c334-edd7-4d26-dad7-a5b0352efd29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoseIt data is already up to date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Apple data"
      ],
      "metadata": {
        "id": "vZ8ieSLKI-yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse export.xml"
      ],
      "metadata": {
        "id": "tLqUP_Noonuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apple_export_to_csv(apple_export_path: str,\n",
        "                        parsed_apple_path: str) -> None:\n",
        "\n",
        "  with open(parsed_apple_path, 'w') as f_out:\n",
        "    writer = csv.writer(f_out)\n",
        "    columns = [\"startDate\", \"endDate\", \"type\", \"unit\", \"value\", \"source\"]\n",
        "    writer.writerow(columns)\n",
        "\n",
        "    with open(apple_export_path, 'r') as f_in:\n",
        "      for _, elem in iterparse(f_in):\n",
        "        if elem.tag == 'Record':\n",
        "          row = elem.attrib\n",
        "          try:\n",
        "            writer.writerow(\n",
        "                [row.get(u'startDate', ''),\n",
        "                 row.get(u'endDate', ''),\n",
        "                 row.get(u'type', ''),\n",
        "                 row.get(u'unit', ''),\n",
        "                 row.get(u'value', ''),\n",
        "                 row.get(u'sourceName', '')]\n",
        "            )\n",
        "          except KeyError as e:\n",
        "            print(f\"KeyError: {e}\")\n",
        "\n",
        "  parsed_apple = pd.read_csv(parsed_apple_path, dtype={'unit' : 'str'})\n",
        "  parsed_apple.drop_duplicates(inplace=True)\n",
        "  parsed_apple.to_csv(parsed_apple_path,\n",
        "                      index=False,\n",
        "                      header=True)"
      ],
      "metadata": {
        "id": "_objKqWrNR6R"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_apple_directory = find_latest_path(_APPLE_ROOT_)\n",
        "apple_export_file = os.path.join(latest_apple_directory, 'export.xml')\n",
        "\n",
        "if not os.path.exists(apple_export_file):\n",
        "  raise Exception(f\"No export.xml file found in the Apple folder {latest_apple_directory}\")\n",
        "elif apple_export_file == get_last_processed('Apple CSV'):\n",
        "  print(\"Apple csv is already up to date\")\n",
        "else:\n",
        "  print(f\"importing Apple export.xml in folder: {latest_apple_directory}\")\n",
        "\n",
        "  parsed_apple_file = os.path.join(_AUXILIARY_ROOT_, 'apple_export.csv')\n",
        "  apple_export_to_csv(apple_export_file, parsed_apple_file)\n",
        "  update_last_processed('Apple CSV', apple_export_file)"
      ],
      "metadata": {
        "id": "QBAJFDMPLmRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd97c5d-38e6-4824-a7b5-76c3ae499130"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Apple export.xml in folder: /content/drive/My Drive/Health Data/Apple/apple_health_export_20240903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-601de5bfddcf>:25: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  parsed_apple = pd.read_csv(parsed_apple_path, dtype={'unit' : 'str'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple Clinical Records"
      ],
      "metadata": {
        "id": "YyH0rN2FPRH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sandardize lab names\n",
        "\"\"\"\n",
        "This is a list of tupples with regular expressions and the names that they should be\n",
        "standardized to.\n",
        "\"\"\"\n",
        "lab_name_patterns = [\n",
        "    (r'^(basophils|baso|% basophils)(?!.*abs.*)', 'basophils'),\n",
        "    (r'^(eosinophils|eos|% eosinophils)(?!.*abs.*)', 'eosinophils'),\n",
        "    (r'^(lymphocytes|lymphs|% lymphocytes)(?!.*abs.*)', 'lymphocytes'),\n",
        "    (r'^(monocytes|% monocytes)(?!.*abs.*)', 'monocytes'),\n",
        "    (r'^(immature granulocytes|%.*immat.*)(?!.*abs.*)', 'immature granulocytes'),\n",
        "    (r'(^(abs.*)+baso.*|^baso.*abs.*)', 'absolute basophils'),\n",
        "    (r'(^(abs.*)+eos.*|^eos.*abs.*)', 'absolute eosinophils'),\n",
        "    (r'(^(abs.*)+neutro.*|^neutro.*abs.*)', 'absolute neutrophils'),\n",
        "    (r'^(neutrophils|%.*neutroph.*)(?!.*abs.*)', 'neutrophils'),\n",
        "    (r'(^(abs.*)+monocy.*|^monocy.*abs.*)', 'absolute monocytes'),\n",
        "    (r'(^(abs.*)+lymph.*|^lymph.*abs.*)', 'absolute lymphocites'),\n",
        "    (r'(^(abs.*)+immat.*|^immat.*abs.*)', 'absolute immature granulocytes'),\n",
        "    (r'^albumin[^/]', 'albumin'),\n",
        "    (r'^a/g.*', 'albumin/globulin ratio'),\n",
        "    (r'^alkaline phosphatase.*', 'alkaline phosphatase'),\n",
        "    (r'^alt.*', 'alt'),\n",
        "    (r'^ast.*', 'ast'),\n",
        "    (r'.*bilirubin.*', 'bilirubin'),\n",
        "    (r'.*urea nit.*', 'bun'),\n",
        "    (r'^calcium.*', 'calcium'),\n",
        "    (r'.*dioxide.*', 'carbon dioxide'),\n",
        "    (r'^chloride.*', 'chloride'),\n",
        "    (r'^creatinine.*', 'creatinine'),\n",
        "    (r'^cholesterol.*', 'cholesterol'),\n",
        "    (r'.*cortisol.*', 'cortisol'),\n",
        "    (r'.*creatine kin.*', 'creatine kinase'),\n",
        "    (r'^globulin.*', 'globulin'),\n",
        "    (r'^glucose.*', 'glucose'),\n",
        "    (r'^hdl.*(c\\Z|chol.*)', 'hdl'),\n",
        "    (r'^ldl.*(c\\Z|chol.*)', 'ldl'),\n",
        "    (r'^hematocrit.*', 'hematocrit'),\n",
        "    (r'^hemoglobin,.*', 'hemoglobin'),\n",
        "    (r'^hemoglobin a.*', 'hba1c'),\n",
        "    (r'^magnesium.*', 'magnesium'),\n",
        "    (r'^mch.*', 'mch'),\n",
        "    (r'^mchc.*', 'mchc'),\n",
        "    (r'^mcv.*', 'mcv'),\n",
        "    (r'^mpv.*', 'mpv'),\n",
        "    (r'^rdw.*', 'rdw'),\n",
        "    (r'^nhdlc', 'non hdl cholesterol'),\n",
        "    (r'.*platelet.*', 'platelets'),\n",
        "    (r'^potassium.*', 'potassium'),\n",
        "    (r'.*protein.*tot.*', 'protein, total'),\n",
        "    (r'^rbc.*', 'red blood cell count'),\n",
        "    (r'^sodium.*', 'sodium'),\n",
        "    (r'^testosterone.*', 'testosterone'),\n",
        "    (r'^triglyc.*', 'triglycerides'),\n",
        "    (r'^vitamin d.*', 'vitamin d'),\n",
        "    (r'^wbc.*', 'white blood cell count')\n",
        "]\n",
        "\n",
        "unit_patterns = [\n",
        "    (r'^notyet', '')\n",
        "]\n",
        "\n",
        "def standardize_strings(input_str: str, patterns: list) -> str:\n",
        "  for pattern, replacement in patterns:\n",
        "    if re.search(pattern, input_str, re.IGNORECASE):\n",
        "      return replacement\n",
        "  return input_str\n",
        "\n",
        "def standardize_names(lab_name:str) -> str:\n",
        "  return standardize_strings(lab_name, lab_name_patterns)\n",
        "\n",
        "def standardize_units(unit:str) -> str:\n",
        "  return standardize_strings(unit, unit_patterns)"
      ],
      "metadata": {
        "id": "X2YWxCj0AzdZ"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_lab_observation(data: dict) -> dict:\n",
        "  \"\"\"\n",
        "  Takes a dictionary like data from a json file from Apple Health that\n",
        "  contains clinical data and returns a dictionary with the\n",
        "  information from a blood test observation.\n",
        "\n",
        "  Args:\n",
        "    data: A dictionary like data from a json file from Apple Health that\n",
        "          contains clinical data.\n",
        "  Returns:\n",
        "    A dictionary with the information from a blood test observation, with the\n",
        "    fields:\n",
        "      date: The date of the observation\n",
        "      name: The name of the observation\n",
        "      value: The value of the observation\n",
        "      unit: The unit of the observation\n",
        "      low_ref: The low reference range of the observation\n",
        "      high_ref: The high reference range of the observation\n",
        "      code: The code of the observation\n",
        "  \"\"\"\n",
        "  obs_date = ''\n",
        "  obs_name = ''\n",
        "  obs_value = ''\n",
        "  obs_unit = ''\n",
        "  low_ref = ''\n",
        "  high_ref = ''\n",
        "  code = ''\n",
        "\n",
        "  obs_date = data.get('effectiveDateTime', '')\n",
        "\n",
        "  # check if it's a lab or a vital\n",
        "  category = data.get('category', {}).get('coding', [])\n",
        "  if len(category) > 0:\n",
        "    category = category[0].get('code')\n",
        "\n",
        "  if category == 'laboratory':\n",
        "    # get rid of any \"valueString\" observations\n",
        "    if 'valueString' in data:\n",
        "      return {}\n",
        "    # get the name and code\n",
        "    obs_name = data.get('code', {}).get('text', '')\n",
        "    obs_code = data.get('code', {}).get('coding', [])\n",
        "    if len(obs_code) > 0:\n",
        "      code = obs_code[0].get('code', '')\n",
        "      if obs_name == '':\n",
        "        obs_name = obs_code[0].get('display', '')\n",
        "\n",
        "    # get the value, unit and reference range\n",
        "    obs_value = data.get('valueQuantity', {}).get('value', '')\n",
        "    if obs_value == '':\n",
        "      return {}\n",
        "    obs_unit = data.get('valueQuantity', {}).get('unit', '')\n",
        "\n",
        "    reference = data.get('referenceRange', [])\n",
        "    if len(reference) > 0:\n",
        "      low_ref = reference[0].get('low', {}).get('value', '')\n",
        "      high_ref = reference[0].get('high', {}).get('value', '')\n",
        "  elif category == 'vital-signs':\n",
        "    # skip vital-signs\n",
        "    pass\n",
        "  else:\n",
        "    print(f\"not processing: {data}\")\n",
        "    return {}\n",
        "\n",
        "  return {'date': obs_date,\n",
        "          'lab': obs_name,\n",
        "          'value': obs_value,\n",
        "          'unit': obs_unit,\n",
        "          'low_ref': low_ref,\n",
        "          'high_ref': high_ref,\n",
        "          'code': code}"
      ],
      "metadata": {
        "id": "CEV3giFO_o4e"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_ehr_observation(data: dict) -> dict:\n",
        "  \"\"\"\n",
        "  Takes a dictionary like data from a json file from Apple Health that\n",
        "  contains clinical data and returns a dictionary with the\n",
        "  information from a medication, immunization, condition or allergy observation.\n",
        "\n",
        "  Args:\n",
        "    data: A dictionary like data from a json file from Apple Health that\n",
        "          contains clinical data.\n",
        "  Returns:\n",
        "  A dictionary with the information from a medication, immunization, condition or allergy observation, with the\n",
        "  fields:\n",
        "      start_date: The start date of the observation\n",
        "      end_date: The end date of the observation\n",
        "      description: The description of the observation\n",
        "      type: The type of the observation (medication, immunization, condition or allergy)\n",
        "      code: The code of the observation\n",
        "  \"\"\"\n",
        "  start_date = ''\n",
        "  obs_type = ''\n",
        "  description = ''\n",
        "  code = ''\n",
        "\n",
        "  if data.get('resourceType') == 'MedicationStatement':\n",
        "    obs_type = 'medication'\n",
        "    if 'contained' in data:\n",
        "      for item in data.get('contained'):\n",
        "        if not type(item) == dict:\n",
        "          raise Exception(f\"Unrecognized format for MedicationStatement['contained']: {data.get('contained')}\")\n",
        "        description = item['code']['text']\n",
        "        if 'coding' in item['code']:\n",
        "          code = item['code']['coding'][0]['code']\n",
        "\n",
        "    if 'medicationCodeableConcept' in data:\n",
        "      description = data['medicationCodeableConcept']['text']\n",
        "      code = data['medicationCodeableConcept']['coding'][0]['code']\n",
        "\n",
        "    if 'effectivePeriod' in data:\n",
        "      if 'start' in data['effectivePeriod']:\n",
        "        start_date = data['effectivePeriod']['start']\n",
        "      if 'end' in data['effectivePeriod']:\n",
        "        start_date = data['effectivePeriod']['end']\n",
        "\n",
        "  elif data.get('resourceType') == 'Immunization':\n",
        "    obs_type = 'immunization'\n",
        "    if 'date' in data:\n",
        "      start_date = data['date']\n",
        "    if 'vaccineCode' in data:\n",
        "      description = data['vaccineCode']['coding'][0]['display']\n",
        "      code = data['vaccineCode']['coding'][0]['code']\n",
        "\n",
        "  elif data.get('resourceType') == 'MedicationOrder':\n",
        "    obs_type = 'medication'\n",
        "    if 'dateWritten' in data:\n",
        "      start_date = data.get('dateWritten')\n",
        "    if 'contained' in data:\n",
        "      for item in data.get('contained'):\n",
        "        if not type(item) == dict:\n",
        "          raise Exception(f\"Unrecognized format for medicationOrder['contained']: {data.get('contained')}\")\n",
        "        description = item['code']['text']\n",
        "        code = item['code']['coding'][0]['code']\n",
        "\n",
        "  elif data.get('resourceType') == 'AllergyIntolerance':\n",
        "    obs_type = 'allergy'\n",
        "    if 'recordedDate' in data:\n",
        "      start_date = data['recordedDate']\n",
        "    if 'substance' in data:\n",
        "      description = data['substance']['text']\n",
        "      if 'coding' in data['substance']:\n",
        "        code = data['substance']['coding'][0]['code']\n",
        "\n",
        "  elif data.get('resourceType') == 'Condition':\n",
        "    obs_type = 'condition'\n",
        "    if 'onsetDateTime' in data:\n",
        "      start_date = data['onsetDateTime']\n",
        "    if 'code' in data:\n",
        "      if 'coding' in data['code']:\n",
        "        description = data['code']['coding'][0]['display']\n",
        "        code = data['code']['coding'][0]['code']\n",
        "      else:\n",
        "        description = data['code']['text']\n",
        "  else:\n",
        "    return {}\n",
        "\n",
        "  return {'start_date': start_date,\n",
        "          'description': description,\n",
        "          'type': obs_type,\n",
        "          'code': code}"
      ],
      "metadata": {
        "id": "1LlhGs3Zy6za"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_apple_ehr(origin_dir: str) -> list:\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  if not os.path.exists(origin_dir):\n",
        "    print(f\"Watch out! {origin_dir} does not exists\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  lab_list = []\n",
        "  observation_list = []\n",
        "\n",
        "  for file_path in os.listdir(origin_dir):\n",
        "    if not file_path.endswith('.json'):\n",
        "      continue\n",
        "    with open(os.path.join(origin_dir, file_path),'r') as f_in:\n",
        "      data = json.load(f_in)\n",
        "\n",
        "    if data.get('resourceType') in ['Patient', 'Procedure']:\n",
        "      pass\n",
        "    elif data.get('resourceType') in ['MedicationStatement',\n",
        "                                      'Immunization',\n",
        "                                      'MedicationOrder',\n",
        "                                      'AllergyIntolerance',\n",
        "                                      'Condition']:\n",
        "      observation = process_ehr_observation(data)\n",
        "      if len(observation) > 0:\n",
        "        observation_list.append(observation)\n",
        "    elif data.get('resourceType') == 'Observation':\n",
        "      blood_test = process_lab_observation(data)\n",
        "      if len(blood_test) > 0:\n",
        "        lab_list.append(blood_test)\n",
        "    elif data.get('resourceType') == 'DiagnosticReport':\n",
        "      report = data.get('contained', [])\n",
        "      for item in report:\n",
        "        blood_test = process_lab_observation(item)\n",
        "        if len(blood_test) > 0:\n",
        "          lab_list.append(blood_test)\n",
        "    else:\n",
        "      print(f\"not processing: {file_path} with {data.get('resourceType')}\")\n",
        "\n",
        "  labs = pd.DataFrame(lab_list)\n",
        "  labs['date'] = pd.to_datetime(labs['date'], format='mixed', utc=True)\n",
        "  labs['date'] = labs['date'].dt.tz_localize(None)\n",
        "  labs['lab'] = labs['lab'].str.lower()\n",
        "  labs['lab'] = labs['lab'].apply(standardize_names)\n",
        "  labs['value'] = pd.to_numeric(labs['value'], errors='coerce')\n",
        "  labs = labs.drop_duplicates(subset=['date', 'lab', 'value'])\n",
        "  labs['unit'] = labs['unit'].apply(standardize_units)\n",
        "  labs = labs.sort_values(by=['date', 'lab']).reset_index(drop=True)\n",
        "\n",
        "  observations = pd.DataFrame(observation_list)\n",
        "  observations['start_date'] = pd.to_datetime(observations['start_date'],\n",
        "                                              format='mixed', utc=True)\n",
        "  observations['start_date'] = observations['start_date'].dt.tz_localize(None)\n",
        "  observations = observations.sort_values(by=['start_date']).reset_index(drop=True)\n",
        "  return [labs, observations]"
      ],
      "metadata": {
        "id": "QdTEdLAHpDk0"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_apple_directory = find_latest_path(_APPLE_ROOT_)\n",
        "apple_ehr_directory = os.path.join(latest_apple_directory, 'clinical-records')\n",
        "\n",
        "if not os.path.exists(apple_ehr_directory):\n",
        "  raise Exception(f\"No 'clinical-records' directory in {latest_apple_directory}\")\n",
        "elif apple_ehr_directory == get_last_processed('Apple EHR'):\n",
        "  print(\"Apple EHR is already up to date\")\n",
        "else:\n",
        "  print(f\"importing Apple EHR from: {apple_ehr_directory}\")\n",
        "\n",
        "  labs, ehr = parse_apple_ehr(apple_ehr_directory)\n",
        "  if not ehr.empty:\n",
        "    parsed_apple_ehr_file = os.path.join(_AUXILIARY_ROOT_, 'ehr.csv')\n",
        "    with open(parsed_apple_ehr_file, 'w') as f_out:\n",
        "      ehr.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "  if not labs.empty:\n",
        "    parsed_apple_blood_labs_file = os.path.join(_AUXILIARY_ROOT_, 'blood_labs.csv')\n",
        "    with open(parsed_apple_blood_labs_file, 'w') as f_out:\n",
        "      labs.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "  update_last_processed('Apple EHR', apple_export_file)"
      ],
      "metadata": {
        "id": "_fAB625S3jGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbca039-e592-495a-acd6-6be293011530"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Apple EHR from: /content/drive/My Drive/Health Data/Apple/apple_health_export_20240903/clinical-records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic HKQuantity values in export.html"
      ],
      "metadata": {
        "id": "thwrpaH2MCXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'HKQuantityTypeIdentifierPeripheralPerfusionIndex',\n",
        "# 'HKQuantityTypeIdentifierDietaryWater',\n",
        "# 'HKQuantityTypeIdentifierHeight',\n",
        "# 'HKQuantityTypeIdentifierOxygenSaturation',\n",
        "# 'HKQuantityTypeIdentifierWalkingHeartRateAverage',\n",
        "# 'HKQuantityTypeIdentifierBloodPressureSystolic',\n",
        "# 'HKQuantityTypeIdentifierBloodPressureDiastolic',\n",
        "# 'HKQuantityTypeIdentifierRespiratoryRate',\n",
        "# 'HKQuantityTypeIdentifierBodyTemperature',\n",
        "# 'HKQuantityTypeIdentifierDistanceWalkingRunning',\n",
        "# 'HKQuantityTypeIdentifierWalkingDoubleSupportPercentage',\n",
        "# 'HKQuantityTypeIdentifierSixMinuteWalkTestDistance',\n",
        "# 'HKQuantityTypeIdentifierAppleStandTime',\n",
        "# 'HKQuantityTypeIdentifierWalkingAsymmetryPercentage',\n",
        "# 'HKQuantityTypeIdentifierStairAscentSpeed',\n",
        "# 'HKQuantityTypeIdentifierStairDescentSpeed',\n",
        "# 'HKQuantityTypeIdentifierAtrialFibrillationBurden',\n",
        "# 'HKQuantityTypeIdentifierAppleWalkingSteadiness',\n",
        "# 'HKQuantityTypeIdentifierAppleSleepingWristTemperature',\n",
        "# 'HKQuantityTypeIdentifierRunningStrideLength',\n",
        "# 'HKQuantityTypeIdentifierRunningVerticalOscillation',\n",
        "# 'HKQuantityTypeIdentifierRunningGroundContactTime',\n",
        "# 'HKQuantityTypeIdentifierHeartRateRecoveryOneMinute',\n",
        "# 'HKQuantityTypeIdentifierRunningPower',\n",
        "# 'HKQuantityTypeIdentifierEnvironmentalSoundReduction',\n",
        "# 'HKQuantityTypeIdentifierRunningSpeed',\n",
        "# 'HKQuantityTypeIdentifierPhysicalEffort',\n",
        "# 'HKCategoryTypeIdentifierAudioExposureEvent',\n",
        "# 'HKQuantityTypeIdentifierAppleExerciseTime',\n",
        "# 'HKQuantityTypeIdentifierEnvironmentalAudioExposure',\n",
        "# 'HKQuantityTypeIdentifierTimeInDaylight',\n",
        "# 'HKQuantityTypeIdentifierHeadphoneAudioExposure',"
      ],
      "metadata": {
        "id": "Wx-QP40HWwqo"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO:\n",
        "# Need to extract metadata from these from original xml to get further metrics\n",
        "# HKCategoryTypeIdentifierSleepAnalysis\n",
        "# HKCategoryTypeIdentifierAppleStandHour"
      ],
      "metadata": {
        "id": "qhtIkWKz2zs6"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple Wearable Data"
      ],
      "metadata": {
        "id": "0Pdxz8MlLa3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apple_get_type(type_id: str,\n",
        "                   sources: list,\n",
        "                   apple_df: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Extract a specific type of data from the Apple DataFrame. Device names can be\n",
        "  provided to filter the data by source.\n",
        "\n",
        "  params:\n",
        "  type_id: They type o extract\n",
        "  sources: Name of the devices that we want to extract\n",
        "  apple_df: The Apple DataFrame\n",
        "\n",
        "  returns:\n",
        "  DataFrame with the extracted data\n",
        "  \"\"\"\n",
        "  if 'type' in apple_df.columns:\n",
        "    if type_id not in apple_df['type'].unique():\n",
        "      raise ValueError(f\"Field '{type_id}' not found in DataFrame.\")\n",
        "    else:\n",
        "      mask = apple_df['type'] == type_id\n",
        "      if len(sources) > 0:\n",
        "        mask = mask & apple_df['source'].isin(sources)\n",
        "\n",
        "      df = apple_df[mask].copy()\n",
        "      if not df.empty: # clean up dates, sort and convert value to number\n",
        "        df['startDate'] = pd.to_datetime(df['startDate'], format='mixed')\n",
        "        df['endDate'] = pd.to_datetime(df['endDate'], format='mixed')\n",
        "        df = df.sort_values(by=['startDate']).reset_index(drop=True)\n",
        "        df.drop(['type'], axis=1, inplace=True)\n",
        "        df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
        "      return df\n",
        "  else:\n",
        "    raise ValueError(f\"Cannot recognize format of Apple DataFrame\")"
      ],
      "metadata": {
        "id": "Jk1xOK3PeYgC"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_apple_file = os.path.join(_AUXILIARY_ROOT_, 'apple_export.csv')\n",
        "parsed_apple = pd.read_csv(parsed_apple_file,\n",
        "                           dtype={'unit' : 'str'},\n",
        "                           low_memory=False)"
      ],
      "metadata": {
        "id": "9NzYcG1AxXNk"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### STEPS\n",
        "apple_steps = apple_get_type('HKQuantityTypeIdentifierStepCount',\n",
        "                             ['Javier’s Apple\\xa0Watch', 'Xavi’s Apple\\xa0Watch'],\n",
        "                             parsed_apple)\n",
        "\n",
        "if not apple_steps.empty:\n",
        "  apple_steps.rename(columns={'value':'steps'}, inplace=True)\n",
        "\n",
        "  # compute timedelta and step_rate ['steps/min']\n",
        "  apple_steps['timedelta'] = apple_steps['endDate'] - apple_steps['startDate']\n",
        "  apple_steps['timedelta'] = apple_steps['timedelta'].dt.total_seconds() / 60\n",
        "  apple_steps['step_rate/min'] = apple_steps['steps'] / apple_steps['timedelta']\n",
        "\n",
        "  apple_steps['date'] = apple_steps['startDate'].dt.date\n",
        "\n",
        "  apple_daily_steps = apple_steps.groupby('date')['steps'].sum().reset_index()"
      ],
      "metadata": {
        "id": "GJxhwZj9UN_2"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### WALKING SPEED\n",
        "apple_walking_speed = apple_get_type('HKQuantityTypeIdentifierWalkingSpeed',\n",
        "                                     ['Javier’s Apple\\xa0Watch',\n",
        "                                      'Xavi’s Apple\\xa0Watch'],\n",
        "                                     parsed_apple)\n",
        "\n",
        "if not apple_walking_speed.empty:\n",
        "  apple_walking_speed.rename(columns={'value':'walking_speed mi/hr'})\n",
        "  apple_walking_speed['date'] = apple_walking_speed['startDate'].dt.date\n",
        "\n",
        "  # some days have multiple walking speed estimates. Average them out\n",
        "  apple_walking_speed = apple_walking_speed.groupby('date')['walking_speed mi/hr'].mean().reset_index()"
      ],
      "metadata": {
        "id": "vwjqqHCEeUud"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### RHR: RESTING HEART RATE\n",
        "apple_rhr = apple_get_type('HKQuantityTypeIdentifierRestingHeartRate',\n",
        "                           ['Javier’s Apple\\xa0Watch', 'Xavi’s Apple\\xa0Watch'],\n",
        "                           parsed_apple)\n",
        "if not apple_rhr.empty:\n",
        "  apple_rhr.rename(columns={'value':'rhr'}, inplace=True)\n",
        "  apple_rhr['date'] = apple_rhr['startDate'].dt.date\n",
        "\n",
        "  # some days have multiple RHR estimates. Average them out\n",
        "  apple_rhr = apple_rhr.groupby('date')['rhr'].mean().reset_index()"
      ],
      "metadata": {
        "id": "DHBSsyaTTEnU"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### WALKING STEP LENGTH\n",
        "apple_step_length = apple_get_type('HKQuantityTypeIdentifierWalkingStepLength',\n",
        "                                   ['Javier’s Apple\\xa0Watch', 'Xavi’s Apple\\xa0Watch'],\n",
        "                                   parsed_apple)\n",
        "if not apple_step_length.empty:\n",
        "  apple_step_length.rename(columns={'value':'step_length'}, inplace=True)"
      ],
      "metadata": {
        "id": "p2WxvPZXqkSK"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### WEIGHT\n",
        "apple_weight = apple_get_type('HKQuantityTypeIdentifierBodyMass', [], parsed_apple)\n",
        "if not apple_weight.empty:\n",
        "  apple_weight.rename(columns={'value':'weight'}, inplace=True)\n",
        "  apple_weight['date'] = apple_weight['startDate'].dt.date\n",
        "  # some days have multiple weight estimates. Average them out\n",
        "  apple_weight = apple_weight.groupby('date')['weight'].mean().reset_index()\n",
        "\n",
        "apple_bmi = apple_get_type('HKQuantityTypeIdentifierBodyMassIndex',[], parsed_apple)\n",
        "if not apple_bmi.empty:\n",
        "  apple_bmi.rename(columns={'value':'bmi'}, inplace=True)\n",
        "  apple_bmi['date'] = apple_bmi['startDate'].dt.date\n",
        "  # some days have multiple weight estimates. Average them out\n",
        "  apple_bmi = apple_bmi.groupby('date')['bmi'].mean().reset_index()\n",
        "apple_weight = pd.merge(apple_weight, apple_bmi, on='date', how='outer')\n",
        "\n",
        "apple_body_fat = apple_get_type('HKQuantityTypeIdentifierBodyFatPercentage',[],parsed_apple)\n",
        "if not apple_body_fat.empty:\n",
        "  apple_body_fat.rename(columns={'value':'body_fat'}, inplace=True)\n",
        "  apple_body_fat['date'] = apple_body_fat['startDate'].dt.date\n",
        "  # some days have multiple weight estimates. Average them out\n",
        "  apple_body_fat = apple_body_fat.groupby('date')['body_fat'].mean().reset_index()\n",
        "apple_weight = pd.merge(apple_weight, apple_body_fat, on='date', how='outer')\n",
        "\n",
        "apple_lean = apple_get_type('HKQuantityTypeIdentifierLeanBodyMass',[], parsed_apple)\n",
        "if not apple_lean.empty:\n",
        "  apple_lean.rename(columns={'value':'lean_body'}, inplace=True)\n",
        "  apple_lean['date'] = apple_lean['startDate'].dt.date\n",
        "  # some days have multiple weight estimates. Average them out\n",
        "  apple_lean = apple_lean.groupby('date')['lean_body'].mean().reset_index()\n",
        "apple_weight = pd.merge(apple_weight, apple_lean, on='date', how='outer')\n",
        "\n",
        "apple_weight = apple_weight.sort_values(by=['date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DyCMDCfvsR4t"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### HEART RATE\n",
        "apple_hr = apple_get_type('HKQuantityTypeIdentifierHeartRate', [], parsed_apple)\n",
        "\n",
        "if not apple_hr.empty:\n",
        "  apple_hr.rename(columns={'value':'hr'}, inplace=True)\n",
        "  apple_hr['date'] = apple_hr['startDate'].dt.date"
      ],
      "metadata": {
        "id": "MqKpnLnu0inW"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### HRV [SDNN]\n",
        "apple_hrv_sdnn = apple_get_type('HKQuantityTypeIdentifierHeartRateVariabilitySDNN',\n",
        "                                ['Javier’s Apple\\xa0Watch', 'Xavi’s Apple\\xa0Watch'],\n",
        "                                parsed_apple)\n",
        "\n",
        "if not apple_hrv_sdnn.empty:\n",
        "  apple_hrv_sdnn.rename(columns={'value':'hrv_sdnn'}, inplace=True)\n",
        "  apple_hrv_sdnn['date'] = apple_hrv_sdnn['startDate'].dt.date\n",
        "\n",
        "  apple_daily_hrv_sdnn = apple_hrv_sdnn.groupby('date')['hrv_sdnn'].median().reset_index()"
      ],
      "metadata": {
        "id": "89cvaGxP2a6b"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CALORIES BURNED\n",
        "apple_active_calories = apple_get_type('HKQuantityTypeIdentifierActiveEnergyBurned',\n",
        "                                       [],\n",
        "                                       parsed_apple)\n",
        "\n",
        "if not apple_active_calories.empty:\n",
        "  apple_active_calories.rename(columns={'value':'active_calories'}, inplace=True)\n",
        "  apple_active_calories['date'] = apple_active_calories['startDate'].dt.date\n",
        "\n",
        "  apple_daily_active_calories = apple_active_calories.groupby('date')['active_calories'].sum().reset_index()\n",
        "\n",
        "\n",
        "apple_basal_calories = apple_get_type('HKQuantityTypeIdentifierBasalEnergyBurned',\n",
        "                                       [],\n",
        "                                       parsed_apple)\n",
        "\n",
        "if not apple_basal_calories.empty:\n",
        "  apple_basal_calories.rename(columns={'value':'basal_calories'}, inplace=True)\n",
        "  apple_basal_calories['date'] = apple_basal_calories['startDate'].dt.date\n",
        "\n",
        "  apple_daily_basal_calories = apple_basal_calories.groupby('date')['basal_calories'].sum().reset_index()\n",
        "\n",
        "apple_daily_burned_calories = pd.merge(apple_daily_active_calories,\n",
        "                                       apple_daily_basal_calories,\n",
        "                                       on='date',\n",
        "                                       how='outer')\n",
        "\n",
        "apple_daily_burned_calories['basal_calories'].fillna(0, inplace=True)\n",
        "apple_daily_burned_calories['active_calories'].fillna(0, inplace=True)\n",
        "apple_daily_burned_calories['burned_calories'] = apple_daily_burned_calories['active_calories'] + apple_daily_burned_calories['basal_calories']\n",
        "\n",
        "apple_daily_burned_calories = apple_daily_burned_calories.sort_values(by=['date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "GqmK0PfT4kAp"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### WAIST CIRCUMFERENCE\n",
        "apple_waist = apple_get_type('HKQuantityTypeIdentifierWaistCircumference',\n",
        "                             [],\n",
        "                             parsed_apple)\n",
        "\n",
        "if not apple_waist.empty:\n",
        "  apple_waist.rename(columns={'value':'waist_circ'}, inplace=True)\n",
        "  apple_waist['date'] = apple_waist['startDate'].dt.date\n",
        "  apple_waist.drop(['startDate', 'endDate'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "jvn4V8r-8BE_"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### VO2_MAX\n",
        "apple_vo2_max = apple_get_type('HKQuantityTypeIdentifierVO2Max',\n",
        "                               [],\n",
        "                               parsed_apple)\n",
        "\n",
        "if not apple_vo2_max.empty:\n",
        "  apple_vo2_max.rename(columns={'value':'vo2_max'}, inplace=True)\n",
        "  apple_vo2_max['date'] = apple_vo2_max['startDate'].dt.date\n",
        "\n",
        "  apple_vo2_max = apple_vo2_max.groupby('date')['vo2_max'].mean().reset_index()\n",
        "  apple_vo2_max = apple_vo2_max.sort_values(by=['date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rjZm0bcM879T"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FLIGHTS CLIMBED\n",
        "apple_flights = apple_get_type('HKQuantityTypeIdentifierFlightsClimbed',\n",
        "                               ['Javier’s Apple\\xa0Watch', 'Xavi’s Apple\\xa0Watch'],\n",
        "                               parsed_apple)\n",
        "\n",
        "if not apple_flights.empty:\n",
        "  apple_flights.rename(columns={'value':'flights'}, inplace=True)\n",
        "  apple_flights['date'] = apple_flights['startDate'].dt.date\n",
        "  apple_daily_flights = apple_flights.groupby('date')['flights'].sum().reset_index()\n",
        "  apple_daily_flights = apple_daily_flights.sort_values(by=['date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "pGFU-ofw-cFi"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BLOOD PRESSURE\n",
        "apple_systolic = apple_get_type('HKQuantityTypeIdentifierBloodPressureSystolic',\n",
        "                                [],\n",
        "                                parsed_apple)\n",
        "\n",
        "if not apple_systolic.empty:\n",
        "  apple_systolic.rename(columns={'value':'systolic'}, inplace=True)\n",
        "  apple_systolic['date'] = apple_systolic['startDate'].dt.date\n",
        "  apple_daily_systolic = apple_systolic.groupby('date')['systolic'].mean().reset_index()\n",
        "\n",
        "\n",
        "apple_diastolic = apple_get_type('HKQuantityTypeIdentifierBloodPressureDiastolic',\n",
        "                                  [],\n",
        "                                  parsed_apple)\n",
        "if not apple_diastolic.empty:\n",
        "  apple_diastolic.rename(columns={'value':'diastolic'}, inplace=True)\n",
        "  apple_diastolic['date'] = apple_diastolic['startDate'].dt.date\n",
        "  apple_daily_diastolic = apple_diastolic.groupby('date')['diastolic'].mean().reset_index()\n",
        "\n",
        "apple_blood_pressure = pd.merge(apple_daily_systolic,\n",
        "                                apple_daily_diastolic,\n",
        "                                on='date',\n",
        "                                how='outer')"
      ],
      "metadata": {
        "id": "bjo9jvwG_69o"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Fitbit Data"
      ],
      "metadata": {
        "id": "Zh06PDmVpkI1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_J2jAeN38fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FITBIT API functions"
      ],
      "metadata": {
        "id": "wg3EmGlXcwcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fitbit import Fitbit\n",
        "from fitbit import exceptions\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "class Fitbit_extension(Fitbit):\n",
        "    _rate_limit_remaining = 150\n",
        "    _time_rate_reset = datetime.datetime.now()\n",
        "\n",
        "    def make_request(self, *args, **kwargs):\n",
        "        '''\n",
        "        This should handle data level errors, improper requests, and bad\n",
        "        serialization\n",
        "\n",
        "        It also checks to make sure we are not exceeding the rate limit and waits\n",
        "        by blocking execution until the rate limit is reset.\n",
        "        '''\n",
        "        print(f'Rate limit remaining: {self._rate_limit_remaining} going to make a request')\n",
        "        if self._rate_limit_remaining < 1:\n",
        "            time_to_wait = datetime.datetime.now() - self._time_rate_reset\n",
        "            if time_to_wait > datetime.timedelta(0):\n",
        "                time_to_wait = time_to_wait.total_seconds()\n",
        "                print(f'Rate limit exceeded. Waiting {time_to_wait} seconds')\n",
        "                time.sleep(time_to_wait)\n",
        "\n",
        "        headers = kwargs.get('headers', {})\n",
        "        headers.update({'Accept-Language': self.system})\n",
        "        kwargs['headers'] = headers\n",
        "        method = kwargs.get('method', 'POST' if 'data' in kwargs else 'GET')\n",
        "        response = self.client.make_request(*args, **kwargs)\n",
        "\n",
        "        self._rate_limit_remaining = int(response.headers['Fitbit-Rate-Limit-Remaining'])\n",
        "        self._time_rate_reset = datetime.datetime.now() + datetime.timedelta(seconds=int(response.headers['Fitbit-Rate-Limit-Reset']))\n",
        "\n",
        "        if response.status_code == 202:\n",
        "            return True\n",
        "        if method == 'DELETE':\n",
        "            if response.status_code == 204:\n",
        "                return True\n",
        "            else:\n",
        "                raise exceptions.DeleteError(response)\n",
        "        try:\n",
        "            rep = json.loads(response.content.decode('utf8'))\n",
        "        except ValueError:\n",
        "            raise exceptions.BadResponse\n",
        "        return rep\n",
        "\n",
        "    def get_food_log(self,\n",
        "                     date: datetime.datetime) -> dict:\n",
        "        url = \"{0}/{1}/user/-/foods/log/date/{date_log}.json\".format(*self._get_common_args(),\n",
        "                                                                     date_log=date.strftime('%Y-%m-%d'))\n",
        "        return self.make_request(url) # type: ignore\n",
        "\n",
        "\n",
        "    def get_sleep_range(self: Fitbit, start_date: datetime.datetime , end_date: datetime.datetime) -> dict:\n",
        "        str_start = start_date.strftime('%Y-%m-%d')\n",
        "        str_end = end_date.strftime('%Y-%m-%d')\n",
        "        print(f'Getting sleep data from {str_start} to {str_end}')\n",
        "        url = \"{0}/1.2/user/-/sleep/date/{start_date}/{end_date}.json\".format(self._get_common_args()[0],\n",
        "                                                                              start_date=str_start,\n",
        "                                                                              end_date=str_end)\n",
        "        return self.make_request(url) # type: ignore"
      ],
      "metadata": {
        "id": "4Vse61CMRdp9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_FILE = os.path.join(_HEALTH_ROOT_, 'fitbit_token.json')\n",
        "\n",
        "def save_token(token: dict) -> None:\n",
        "    with open(TOKEN_FILE, 'w') as f:\n",
        "        json.dump(token, f)\n",
        "\n",
        "def load_token():\n",
        "    if os.path.isfile(TOKEN_FILE):\n",
        "        with open(TOKEN_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    else:\n",
        "        raise ValueError(\"Could not find token file\")\n",
        "\n",
        "CLIENT_ID = userdata.get('_FITBIT_CLIENT_ID')\n",
        "CLIENT_SECRET = userdata.get('_FITBIT_SECRET')\n",
        "redirect_uri = 'http://127.0.0.1:8080'\n",
        "_FIRST_FITBIT_DATE_ = datetime.datetime(year=2011, month=5, day=1)\n",
        "_API_TIME_INTERVAL_ = 25 # seconds"
      ],
      "metadata": {
        "id": "cQwPIEvjUrdm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pull Fitbit data"
      ],
      "metadata": {
        "id": "Q4hRDHtpc21_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weight(client,\n",
        "                  weight_file_path: str = os.path.join('data/weight.csv')) -> None:\n",
        "    '''\n",
        "    Updates the weight file with the latest data from Fitbit\n",
        "\n",
        "    params:\n",
        "    client: Fitbit client\n",
        "    weight_file_path: path to the weight file\n",
        "    '''\n",
        "    if os.path.isfile(path=weight_file_path):\n",
        "        df = pd.read_csv(weight_file_path)\n",
        "        last_date = datetime.datetime.strptime(str(df['date'].iloc[-1:].values[0]), '%Y-%m-%d')\n",
        "        print(f'I found the weight file. The last date in it is {last_date}')\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=['bmi', 'date', 'fat', 'logId', 'time', 'weight', 'source'])\n",
        "        last_date = _FIRST_FITBIT_DATE_\n",
        "\n",
        "    yesterday = datetime.datetime.today() - datetime.timedelta(days=1)\n",
        "    end_date = yesterday\n",
        "    print(f'I will pull weight data until {yesterday}')\n",
        "\n",
        "    while ((yesterday - last_date) >= datetime.timedelta(days=1)):\n",
        "        if((yesterday - last_date) > datetime.timedelta(days=30)):\n",
        "            end_date = last_date + datetime.timedelta(days=30)\n",
        "        else:\n",
        "            end_date = yesterday\n",
        "\n",
        "        data = client.get_bodyweight(base_date=last_date, end_date=end_date)\n",
        "        df = pd.concat([df, pd.DataFrame(data['weight'])]).drop_duplicates().reset_index(drop=True) # type: ignore\n",
        "        df.to_csv(weight_file_path, index=False)\n",
        "        last_date = end_date + datetime.timedelta(days=1)\n",
        "        time.sleep(_API_TIME_INTERVAL_)"
      ],
      "metadata": {
        "id": "-K5HNxlwWESK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = load_token()\n",
        "fitbit_client = Fitbit_extension(\n",
        "        client_id=CLIENT_ID,\n",
        "        client_secret=CLIENT_SECRET,\n",
        "        access_token=token['access_token'],\n",
        "        refresh_token=token['refresh_token'],\n",
        "        expires_at=token['expires_at'],\n",
        "        refresh_cb=save_token,\n",
        "        redirect_uri=redirect_uri,\n",
        "        timeout=10)\n",
        "\n",
        "update_weight(fitbit_client, os.path.join(_PARSED_ROOT_, 'fitbit_weight.csv'))\n",
        "# logger.debug(f'I finished updating the weight at {time.asctime()}\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN71F_sbWS_Y",
        "outputId": "5ed772a2-54e1-462a-95f2-3f9b1220c684"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I found the weight file. The last date in it is 2024-08-23 00:00:00\n",
            "I will pull weight data until 2024-08-28 04:50:15.091196\n",
            "Rate limit remaining: 150 going to make a request\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fitbit_client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9N8jJAcbfnB",
        "outputId": "d8060d48-f695-46f2-a658-b50b7a851813"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Fitbit_extension at 0x7a368a00d9f0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latest_fitbit_path = find_latest_path(_FITBIT_ROOT_)\n",
        "\n",
        "if latest_fitbit_path is None:\n",
        "  print(\"No Fitbit data found\")\n",
        "\n",
        "latest_fitbit_path = os.path.join(latest_fitbit_path, 'Fitbit')"
      ],
      "metadata": {
        "id": "W2rBQ9RyQXcw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_csv_files = []\n",
        "for folder in os.listdir(latest_fitbit_path):\n",
        "  folder_path = os.path.join(latest_fitbit_path, folder)\n",
        "  if os.path.isdir(folder_path):\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "    all_csv_files.extend(csv_files)\n",
        "  else:\n",
        "    print(f\"{folder_path} is not a directory\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MwpOWyCsC18",
        "outputId": "b0a5feb1-d06b-499a-b98f-84d6ff08a221"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Stress Score\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Biometrics\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Heart Rate Variability\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Heart Rate\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Atrial Fibrillation ECG\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Daily Readiness\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Paired Devices\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Mindfulness\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Oxygen Saturation (SpO2)\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Sleep\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Sleep Score\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Stress Journal\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Snore and Noise Detect\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Journal Log\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Menstrual Health\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Guided Programs\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Messages\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Temperature\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Active Zone Minutes (AZM)\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Activity Goals\n",
            "Processing folder: /content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Global Export Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in all_csv_files:\n",
        "  if 'Sleep' in x:\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ3nrosntvx_",
        "outputId": "f1883d2b-a377-41a0-cfa8-1cf434402704"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Sleep/Sleep Profile.csv\n",
            "/content/drive/My Drive/Health Data/Fitbit/Takeout/Fitbit/Sleep Score/sleep_score.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_fitbit_data(file_path: str) -> pd.DataFrame:\n",
        "  category_mapping ={\n",
        "      \"Daily Readiness Score\": \"Readiness\",\n",
        "      \"sleep_score\": \"Sleep\",\n",
        "      \"Daily SpO2\": \"SpO2\",\n",
        "      \"Respiratory Rate Summary\": \"Respiration\",\n",
        "      \"Daily Respiratory Rate Summary\": \"Respiration\",\n",
        "      \"Daily Heart Rate Variability Summary\": \"HRV\",\n",
        "      \"Computed Temperature\": \"Stress\",\n",
        "      \"Stress Score\": \"Stress\"\n",
        "  }"
      ],
      "metadata": {
        "id": "44BFjIbnpsi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assemble and Explore Glucose Data"
      ],
      "metadata": {
        "id": "3jGRAcC4qTJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean up CGM data (assmeble, remove partial days, filter)"
      ],
      "metadata": {
        "id": "-ZIQLo0lEVO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abbott_log = pd.read_csv(os.path.join(_AUXILIARY_ROOT_, 'abbott.csv'))\n",
        "nutrisense_glucose = pd.read_csv(os.path.join(_AUXILIARY_ROOT_, 'nutrisense_glucose.csv'))"
      ],
      "metadata": {
        "id": "kfL5dwo-Cq7M"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all glucose data and sort by time\n",
        "abbott_log['glucose'] = pd.to_numeric(abbott_log['glucose'], errors='coerce')\n",
        "abbott_log['time'] = pd.to_datetime(abbott_log['time'])\n",
        "\n",
        "nutrisense_glucose['glucose'] = pd.to_numeric(nutrisense_glucose['glucose'], errors='coerce')\n",
        "nutrisense_glucose['time'] = pd.to_datetime(nutrisense_glucose['time'])\n",
        "\n",
        "glucose_data = pd.concat([abbott_log, nutrisense_glucose])\n",
        "glucose_data = glucose_data.drop_duplicates(subset=['time'],\n",
        "                                            keep=False).reset_index(drop=True)\n",
        "glucose_data = glucose_data.sort_values(by=['time']).reset_index(drop=True)\n",
        "glucose_data['date'] = glucose_data['time'].dt.date"
      ],
      "metadata": {
        "id": "GiGfYi4HESXi"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the days that are only partially recorded in the CGM data\n",
        "# (they're first and last day anyways)\n",
        "grouped = glucose_data.groupby('date')\n",
        "time_spans = grouped['time'].agg(lambda x: x.max() - x.min())\n",
        "\n",
        "min_time_span = pd.Timedelta(hours=22)\n",
        "complete_days = time_spans[time_spans >= min_time_span].index\n",
        "glucose_data = glucose_data[glucose_data['date'].isin(complete_days)]\n",
        "glucose_data = glucose_data.sort_values(by=['time']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "JzzntxHvgpMr"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glucose_data['glucose_filtered'] = glucose_data['glucose']\n",
        "# for day in glucose_data['date'].unique():\n",
        "#   df = glucose_data[glucose_data['date'] == day].copy()\n",
        "#   meal_day = compact_loseit[compact_loseit['Date'] == day]\n",
        "\n",
        "#   kf = KalmanFilter(dim_x=2, dim_z=1)\n",
        "\n",
        "#   # Define the initial state: [glucose level, glucose velocity]\n",
        "#   kf.x = np.array([[df['glucose'].iloc[0]], [0]])  # Initial state\n",
        "\n",
        "#   # State Transition Matrix (F): Model how the state evolves\n",
        "#   dt = 1.0  # time step\n",
        "#   kf.F = np.array([[1, dt],\n",
        "#                   [0, 1]])\n",
        "\n",
        "#   # Observation Matrix (H): Model how measurements relate to the state\n",
        "#   kf.H = np.array([[1, 0]])\n",
        "\n",
        "#   # Process noise covariance (Q): Assumed variance in the process\n",
        "#   kf.Q = 10 * np.array([[1e-5, 0],\n",
        "#                       [0, 1e-5]])\n",
        "\n",
        "#   # Measurement noise covariance (R): Variance in the measurements\n",
        "#   kf.R = np.array([[0.1]])\n",
        "\n",
        "#   # Covariance matrix (P)\n",
        "#   kf.P = np.eye(2) * 0.1\n",
        "\n",
        "#   # Kalman filtering of the glucose measurements\n",
        "#   filtered_glucose = []\n",
        "#   adaptation_factor = 0.1\n",
        "#   for z in df['glucose']:\n",
        "#       kf.predict()   # Predict the next state\n",
        "#       # residual = z - kf.x[0, 0]\n",
        "#       # kf.Q *= (1 + adaptation_factor * np.abs(residual))\n",
        "#       kf.update(z)   # Update the state with the new measurement\n",
        "#       filtered_glucose.append(kf.x[0, 0])\n",
        "\n",
        "#   glucose_data.loc[df.index, 'glucose_filtered'] = filtered_glucose"
      ],
      "metadata": {
        "id": "EXmL39o6mFRs"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assemble meal data\n",
        "\n"
      ],
      "metadata": {
        "id": "HeNcWkKJE6L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loseit_meals = pd.read_csv(os.path.join(_AUXILIARY_ROOT_, 'loseit_meals.csv'))\n",
        "nutrisense_meals = pd.read_csv(os.path.join(_AUXILIARY_ROOT_, 'nutrisense_meals.csv'))"
      ],
      "metadata": {
        "id": "7D9w93pOEJk4"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meals = pd.concat([loseit_meals, nutrisense_meals], ignore_index=True)\n",
        "meals['DateTime'] = pd.to_datetime(meals['DateTime'])\n",
        "meals['Date'] = meals['DateTime'].dt.date\n",
        "meals = meals.sort_values(by=['DateTime']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "CotrVvV6qd0I"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize each day [CGM and days] and join"
      ],
      "metadata": {
        "id": "5f0H7tkQcHYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cgm_daily_stats =  glucose_data.groupby('date')['glucose'].agg(\n",
        "    mean='mean',\n",
        "    median='median',\n",
        "    std='std',\n",
        "    min='min',\n",
        "    max='max')\n",
        "cgm_daily_stats.rename(columns={'mean': 'cgm_mean',\n",
        "                                'median': 'cgm_median',\n",
        "                                'std': 'cgm_std',\n",
        "                                'min': 'cgm_min',\n",
        "                                'max': 'cgm_max'}, inplace=True)"
      ],
      "metadata": {
        "id": "p1VGcDPDcNu0"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_macros =  meals.groupby('Date')[['Calories',\n",
        "                                       'Fat',\n",
        "                                       'Protein',\n",
        "                                       'Carbs',\n",
        "                                       'Sat Fat',\n",
        "                                       'Sugar',\n",
        "                                       'Fiber']].sum()\n",
        "daily_macros['calc_calories'] = daily_macros['Fat'] * 9 + \\\n",
        "                                daily_macros['Protein'] * 4 + \\\n",
        "                                daily_macros['Carbs'] * 4\n",
        "daily_macros['%_fat'] = 9 * daily_macros['Fat'] / daily_macros['calc_calories']\n",
        "daily_macros['%_carbs'] = 4 * daily_macros['Carbs'] / daily_macros['calc_calories']\n",
        "daily_macros['%_proteins'] = 4 * daily_macros['Protein'] / daily_macros['calc_calories']\n",
        "daily_macros['sugar_to_fiber'] = daily_macros['Sugar'] / daily_macros['Fiber']"
      ],
      "metadata": {
        "id": "khZOQAikehG9"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cgm_and_macros = pd.merge(daily_macros, cgm_daily_stats, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "# remove days unrealistically low or high on total calories (e.g. < 1000)\n",
        "cgm_and_macros = cgm_and_macros[cgm_and_macros['Calories'] > 1000].copy()\n",
        "cgm_and_macros = cgm_and_macros[cgm_and_macros['Calories'] < 4000].copy()"
      ],
      "metadata": {
        "id": "SJRC7G59gf0b"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add CGM stats for each meal"
      ],
      "metadata": {
        "id": "zaPllbtAGw74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meals_cgm = meals[meals['Date'].isin(cgm_and_macros.index)]\n",
        "\n",
        "# Go through each meal that has meal and CGM and compute CGM metrics of those meals\n",
        "\"\"\"\n",
        "compute:\n",
        "Peak\n",
        "AUC\n",
        "Delta\n",
        "2h Delta\n",
        "\"\"\"\n",
        "meal_cgm_stats = []\n",
        "for idx, meal in meals_cgm.iterrows():\n",
        "  meal_peak = 0\n",
        "  meal_auc = 0\n",
        "  meal_delta = 0\n",
        "  meal_2h_delta = 0\n",
        "\n",
        "  meal_time = meal['DateTime']\n",
        "  meal_time_end = meal_time + pd.Timedelta(hours=2)\n",
        "  meal_cgm = glucose_data[(glucose_data['time'] > meal_time) & (glucose_data['time'] < meal_time_end)].copy()\n",
        "  meal_cgm['time_diff'] = (meal_cgm['time'] - meal_cgm['time'].iloc[0]).dt.total_seconds() / 3600\n",
        "\n",
        "  if not meal_cgm.empty:\n",
        "    meal_peak = meal_cgm['glucose'].max()\n",
        "    meal_auc = np.trapz(meal_cgm['glucose'], x=meal_cgm['time_diff'])\n",
        "    meal_delta = meal_cgm['glucose'].max() - meal_cgm['glucose'].min()\n",
        "    meal_2h_delta = meal_cgm['glucose'].iloc[-1] - meal_cgm['glucose'].iloc[0]\n",
        "\n",
        "    meal_cgm_stats.append({'DateTime' : meal_time,\n",
        "                           'cgm_peak' : meal_peak,\n",
        "                           'cgm_auc' : meal_auc,\n",
        "                           'cgm_delta' : meal_delta,\n",
        "                           'cgm_2h_delta': meal_2h_delta})\n",
        "\n",
        "meals_cgm = pd.merge(meals_cgm, pd.DataFrame(meal_cgm_stats), on='DateTime', how='left')"
      ],
      "metadata": {
        "id": "NbXcbvMBGVqR"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save meals and glucose data"
      ],
      "metadata": {
        "id": "e-XmA5UEtdlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_macros_cgm_stats = os.path.join(_PARSED_ROOT_, 'daily_macros_cgm_stats.csv')\n",
        "print(f\"Saving daily CGM stats with macros data in path: {out_macros_cgm_stats}\")\n",
        "cgm_and_macros['Date'] = cgm_and_macros.index\n",
        "with open(out_macros_cgm_stats, 'w') as f_out:\n",
        "  cgm_and_macros.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "out_meals_cgm_stats = os.path.join(_PARSED_ROOT_, 'meals_cgm.csv')\n",
        "print(f\"Saving meals with CGM stats data in path: {out_meals_cgm_stats}\")\n",
        "meals_cgm['Date'] = meals_cgm.index\n",
        "with open(out_meals_cgm_stats, 'w') as f_out:\n",
        "  meals_cgm.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "out_glucose_path = os.path.join(_PARSED_ROOT_, 'glucose.csv')\n",
        "print(f\"Saving parsed CGM data in path: {out_glucose_path}\")\n",
        "with open(out_glucose_path, 'w') as f_out:\n",
        "  glucose_data.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "out_meals_path = os.path.join(_PARSED_ROOT_, 'meals.csv')\n",
        "print(f\"Saving parsed meals in path: {out_meals_path}\")\n",
        "with open(out_meals_path, 'w') as f_out:\n",
        "  meals.to_csv(f_out, index=False, header=True)\n",
        "\n",
        "out_daily_macros_path = os.path.join(_PARSED_ROOT_, 'daily_macros.csv')\n",
        "print(f\"Saving daily meals in path: {out_daily_macros_path}\")\n",
        "with open(out_daily_macros_path, 'w') as f_out:\n",
        "  daily_macros.to_csv(f_out, index=True, header=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuZDdnIytcm8",
        "outputId": "bf0f9e37-ac73-4914-8108-61268f129718"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving daily CGM stats with macros data in path: /content/drive/My Drive/Health Data/Parsed/daily_macros_cgm_stats.csv\n",
            "Saving meals with CGM stats data in path: /content/drive/My Drive/Health Data/Parsed/meals_cgm.csv\n",
            "Saving parsed CGM data in path: /content/drive/My Drive/Health Data/Parsed/glucose.csv\n",
            "Saving parsed meals in path: /content/drive/My Drive/Health Data/Parsed/meals.csv\n",
            "Saving daily meals in path: /content/drive/My Drive/Health Data/Parsed/daily_macros.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Daily Summaries and other artifacts"
      ],
      "metadata": {
        "id": "I41csEnELpFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge weight form all sources\n",
        "weight = apple_weight.copy()"
      ],
      "metadata": {
        "id": "6nle1kY3a8bK"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge steps from all sources\n",
        "steps = apple_daily_steps.copy()"
      ],
      "metadata": {
        "id": "xfuVXxPNaz-F"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge RHR from all sources\n",
        "rhr = apple_rhr.copy()"
      ],
      "metadata": {
        "id": "bSOjc_Sab-YQ"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge HRV [SDNN] from all sources\n",
        "sdnn = apple_daily_hrv_sdnn.copy()"
      ],
      "metadata": {
        "id": "xMBY4EBscWcK"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge burned calories info\n",
        "burned_calories = apple_daily_burned_calories.copy()"
      ],
      "metadata": {
        "id": "-Zq8-pqIc7M3"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge flights info\n",
        "flights = apple_daily_flights.copy()"
      ],
      "metadata": {
        "id": "VXOR2mw-AiL_"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge nutrition macros and CGM stats\n",
        "d_m = daily_macros.copy()\n",
        "d_m = d_m.round(3)\n",
        "d_m['date'] = d_m.index\n",
        "d_m = d_m.reset_index(drop=True)\n",
        "\n",
        "cgm_stats = cgm_daily_stats.copy()\n",
        "cgm_stats = cgm_stats.round(1)\n",
        "cgm_stats['date'] = cgm_stats.index\n",
        "cgm_stats = cgm_stats.reset_index(drop=True)\n",
        "\n",
        "macros = pd.merge(d_m, cgm_stats, on='date', how='outer')\n",
        "macros = macros.sort_values(by=['date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "C8zR5bwYdNo8"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a DataFrame with daily metrics containing\n",
        "# steps, sleep metrics, weight, RHR, AZM\n",
        "# apple_steps\n",
        "daily_metrics = pd.merge(weight, steps, on='date', how='outer')\n",
        "daily_metrics = pd.merge(daily_metrics, rhr, on='date', how='outer')\n",
        "daily_metrics = pd.merge(daily_metrics, sdnn, on='date', how='outer')\n",
        "daily_metrics = pd.merge(daily_metrics, burned_calories, on='date', how='outer')\n",
        "daily_metrics = pd.merge(daily_metrics, macros, on='date', how='outer')\n",
        "daily_metrics = pd.merge(daily_metrics, flights, on='date', how='outer')\n",
        "\n",
        "daily_metrics = daily_metrics.sort_values(by=['date']).reset_index(drop=True)\n",
        "with open(os.path.join(_PARSED_ROOT_, 'daily_metrics.csv'), 'w') as f_out:\n",
        "  daily_metrics.to_csv(f_out, index=False, header=True)"
      ],
      "metadata": {
        "id": "1ZRoZVadLu6g"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a DataFrame with daily metrics of food and cgm metrics\n",
        "# fat, protein, glucose mean, etc."
      ],
      "metadata": {
        "id": "ioOjwbxaADAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate DataFrame with meal summary"
      ],
      "metadata": {
        "id": "KEf_zfSGANUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a daily \"text entry\" with the meals, daily metrics and when existing PHR"
      ],
      "metadata": {
        "id": "TDDxB4azAWw9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}